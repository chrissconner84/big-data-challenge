{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrissconner84/big-data-challenge/blob/main/big_data_appliances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BtCOyQ3kChJ",
        "outputId": "6669473d-7833-4cf6-e80d-e02de521b874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Connecting to security.ub\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.2.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3FP8HaLokHui"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BigDataAppliances\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LSw6QzzkMkY",
        "outputId": "074dae3c-556c-404a-8d18-5bcac8aa8aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|         US|   16199106|R203HPW78Z7N4K|B0067WNSZY|     633038551|FGGF3032MW Galler...|Major Appliances|          5|            0|          0|   N|                Y|If you need a new...|What a great stov...| 2015-08-31|\n",
            "|         US|   16374060|R2EAIGVLEALSP3|B002QSXK60|     811766671|Best Hand Clothes...|Major Appliances|          5|            1|          1|   N|                Y|          Five Stars|        worked great| 2015-08-31|\n",
            "|         US|   15322085|R1K1CD73HHLILA|B00EC452R6|     345562728|Supco SET184 Ther...|Major Appliances|          5|            0|          0|   N|                Y|       Fast Shipping|Part exactly what...| 2015-08-31|\n",
            "|         US|   32004835|R2KZBMOFRMYOPO|B00MVVIF2G|     563052763|Midea WHS-160RB1 ...|Major Appliances|          5|            1|          1|   N|                Y|          Five Stars|Love my refrigera...| 2015-08-31|\n",
            "|         US|   25414497| R6BIZOZY6UD01|B00IY7BNUW|     874236579|Avalon Bay Portab...|Major Appliances|          5|            0|          0|   N|                Y|          Five Stars|No more running t...| 2015-08-31|\n",
            "|         US|   36311751|R1MCXZFNF8E7Y0|B0033X29CI|     294467812|Danby  Freestandi...|Major Appliances|          1|            0|          0|   N|                Y|       Piece of Junk|It would not cool...| 2015-08-31|\n",
            "|         US|   30920961|R3EMB3E3ODR6BW|B005R597HA|     183784715|Avanti 110-Volt A...|Major Appliances|          5|            2|          2|   N|                Y|Works awesome for...|Works awesome for...| 2015-08-31|\n",
            "|         US|   52491265| RJTONVTTOPJ5S|B00MO6V8Y0|     960251524|      Danby products|Major Appliances|          5|            0|          0|   N|                Y|          Five Stars|exactly what I wa...| 2015-08-31|\n",
            "|         US|   48166169|R21U5QZ2CQECUM|B00HT39QDI|     992475314|3 Pack Tier1 MSWF...|Major Appliances|          4|            0|          0|   N|                Y|          Four Stars|       AS advertised| 2015-08-31|\n",
            "|         US|   50394924| RL2BBC51H89DH|B00LESFZ52|       1641606|True TSSU-60-16 6...|Major Appliances|          4|            0|          0|   N|                Y|but has poor insu...|It works as adver...| 2015-08-31|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkFiles\n",
        "# Load in user_data.csv from S3 into a DataFrame\n",
        "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Major_Appliances_v1_00.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"amazon_reviews_us_Major_Appliances_v1_00.tsv.gz\"), inferSchema=True, sep='\\t', timestampFormat=\"mm/dd/yy\")\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IifpzN6zkN5Q"
      },
      "source": [
        "## Transform DataFrame to fit coffe_rating table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E72hLCa7kT_D"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kRvAVaeWkXXn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "OJgQqozHkYhz",
        "outputId": "8c0d6b3d-4118-49ac-c199-5cb61cee86f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+-----------------+\n",
            "|    coffee_shop_name|total_ratings|       avg_rating|\n",
            "+--------------------+-------------+-----------------+\n",
            "|Lola Savannah Cof...|            4|              5.0|\n",
            "|The Marvelous Vin...|           10|              5.0|\n",
            "|Ma√±ana Coffee & J...|           33|4.848484848484849|\n",
            "|       Brian's Brew |           45|4.844444444444444|\n",
            "|Third Coast Coffe...|           56|4.821428571428571|\n",
            "|      Flitch Coffee |           28|4.821428571428571|\n",
            "|   Kowabunga Coffee |           16|           4.8125|\n",
            "|Venezia Italian G...|          200|             4.81|\n",
            "|      Legend Coffee |           28|4.714285714285714|\n",
            "|       Fleet Coffee |           57|4.701754385964913|\n",
            "|    My Sweet Austin |           31| 4.67741935483871|\n",
            "|         Dolce Neve |          100|             4.64|\n",
            "|       Holy Grounds |           30|4.633333333333334|\n",
            "|Anderson's Coffee...|          100|             4.62|\n",
            "|Apanas Coffee & B...|          136|4.580882352941177|\n",
            "|  Flat Track Coffee |           63|4.571428571428571|\n",
            "|Friends & Neighbors |           29|4.551724137931035|\n",
            "|Summermoon Coffee...|          100|             4.53|\n",
            "|      Corona Coffee |          100|             4.53|\n",
            "|    Live Oak Market |          100|             4.51|\n",
            "+--------------------+-------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import desc\n",
        "coffee_ratings_df = coffee_ratings_df.withColumnRenamed(\"count(coffee_shop_name)\", \"total_ratings\")\\\n",
        "                                     .withColumnRenamed(\"avg(num_rating)\", \"avg_rating\")\n",
        "coffee_ratings_df.orderBy(desc(\"avg_rating\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnF65nZ1kZAi"
      },
      "source": [
        "## Transform DataFrame to fit date_table table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8gR8zpukhqW"
      },
      "outputs": [],
      "source": [
        "review_df = df.select([\"review_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "iuV6hvV6kkcU",
        "outputId": "646523af-9216-4048-d382-9656ad796579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------+\n",
            "|      date|         review_text|\n",
            "+----------+--------------------+\n",
            "|11/25/2016|1 check-in Love l...|\n",
            "| 12/2/2016|Listed in Date Ni...|\n",
            "|11/30/2016|1 check-in Listed...|\n",
            "|11/25/2016|Very cool vibe! G...|\n",
            "| 12/3/2016|1 check-in They a...|\n",
            "|11/20/2016|1 check-in Very c...|\n",
            "|10/27/2016|2 check-ins Liste...|\n",
            "| 11/2/2016|2 check-ins Love ...|\n",
            "|10/25/2016|1 check-in Ok let...|\n",
            "|11/10/2016|3 check-ins This ...|\n",
            "|10/22/2016|1 check-in Listed...|\n",
            "|11/20/2016|The store has A+ ...|\n",
            "|11/17/2016|1 check-in Listed...|\n",
            "| 12/5/2016|This is such a cu...|\n",
            "|11/13/2016|Beautiful eccentr...|\n",
            "| 11/9/2016|1 check-in Listed...|\n",
            "| 11/6/2016|Really love the v...|\n",
            "|10/25/2016|1 check-in Check ...|\n",
            "|10/15/2016|1 check-in Note: ...|\n",
            "| 12/1/2016|So much aesthetic...|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "review_df = review_df.withColumn(\"date\", regexp_extract(\"review_text\", \"\\d+/\\d+/\\d+\", 0))\\\n",
        "      .withColumn(\"review_text\", regexp_extract(\"review_text\", \"\\d+/\\d+/\\d+(?:\\s)(.*)\", 1))\\\n",
        "      .select([\"date\", \"review_text\"])\\\n",
        "      .dropna()\n",
        "review_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "nVPFb0PzkkTH",
        "outputId": "a5134e88-4f1c-4f67-827d-5590007af543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------+\n",
            "|      date|review_count|\n",
            "+----------+------------+\n",
            "| 8/21/2016|          16|\n",
            "| 6/29/2016|          10|\n",
            "| 8/19/2013|           2|\n",
            "| 2/27/2015|           5|\n",
            "| 7/31/2016|          13|\n",
            "| 3/17/2014|           7|\n",
            "|11/14/2015|          11|\n",
            "| 6/10/2011|           1|\n",
            "|10/10/2009|           1|\n",
            "| 4/27/2014|           1|\n",
            "| 3/27/2009|           1|\n",
            "| 12/8/2011|           1|\n",
            "| 2/21/2014|           2|\n",
            "| 8/31/2015|          10|\n",
            "| 1/15/2015|           3|\n",
            "| 3/16/2012|           1|\n",
            "|  8/9/2016|           4|\n",
            "|11/24/2016|           1|\n",
            "|  8/2/2014|           5|\n",
            "| 3/23/2011|           1|\n",
            "+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "date_df = review_df.groupBy('date').agg({\"date\": \"count\"})\n",
        "date_df = date_df.withColumnRenamed(\"count(date)\", \"review_count\")\n",
        "date_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "E6P2i7rCkjte",
        "outputId": "e0235c48-5dd4-4771-e204-5ed56bbd807a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------+\n",
            "|      date|review_count|\n",
            "+----------+------------+\n",
            "| 10/9/2016|          31|\n",
            "| 9/18/2016|          30|\n",
            "|11/20/2016|          27|\n",
            "| 11/2/2016|          27|\n",
            "| 12/2/2016|          26|\n",
            "| 12/4/2016|          26|\n",
            "| 9/15/2016|          25|\n",
            "| 10/7/2016|          24|\n",
            "| 11/6/2016|          24|\n",
            "| 7/24/2016|          24|\n",
            "| 4/17/2016|          23|\n",
            "|10/25/2016|          23|\n",
            "| 12/3/2016|          23|\n",
            "| 12/1/2016|          23|\n",
            "|  8/7/2016|          22|\n",
            "| 6/27/2016|          22|\n",
            "|  1/4/2016|          21|\n",
            "| 1/17/2016|          21|\n",
            "|11/21/2016|          21|\n",
            "| 8/13/2016|          20|\n",
            "+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "date_df.orderBy(desc(\"review_count\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXQe3s_ElN2U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "big_data_appliances.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
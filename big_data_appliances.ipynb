<<<<<<< HEAD
{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BtCOyQ3kChJ","executionInfo":{"status":"ok","timestamp":1652476170850,"user_tz":420,"elapsed":25514,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"a621c020-41bf-4eb4-a4bd-2e92ad19953c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,195 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,277 kB]\n","Fetched 5,724 kB in 3s (2,135 kB/s)\n","Reading package lists... Done\n"]}],"source":["import os\n","# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.0.3'\n","spark_version = 'spark-3.2.1'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"]},{"cell_type":"code","source":["!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mOauduCXQX1g","executionInfo":{"status":"ok","timestamp":1652476200357,"user_tz":420,"elapsed":566,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"0e36bb33-5bd4-4441-e567-4b425f3de100"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-13 21:10:00--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n","Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n","Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 914037 (893K) [application/java-archive]\n","Saving to: ‘postgresql-42.2.9.jar’\n","\n","postgresql-42.2.9.j 100%[===================>] 892.61K  --.-KB/s    in 0.08s   \n","\n","2022-05-13 21:10:00 (10.6 MB/s) - ‘postgresql-42.2.9.jar’ saved [914037/914037]\n","\n"]}]},{"cell_type":"code","execution_count":26,"metadata":{"id":"3FP8HaLokHui","executionInfo":{"status":"ok","timestamp":1652481551435,"user_tz":420,"elapsed":111,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"BigDataAppliances\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LSw6QzzkMkY","outputId":"4c05b5d1-250a-453e-f651-06d9440983a9","executionInfo":{"status":"ok","timestamp":1652481557053,"user_tz":420,"elapsed":1674,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n","|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n","+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n","|         US|   16199106|R203HPW78Z7N4K|B0067WNSZY|     633038551|FGGF3032MW Galler...|Major Appliances|          5|            0|          0|   N|                Y|If you need a new...|What a great stov...| 2015-08-31|\n","|         US|   16374060|R2EAIGVLEALSP3|B002QSXK60|     811766671|Best Hand Clothes...|Major Appliances|          5|            1|          1|   N|                Y|          Five Stars|        worked great| 2015-08-31|\n","|         US|   15322085|R1K1CD73HHLILA|B00EC452R6|     345562728|Supco SET184 Ther...|Major Appliances|          5|            0|          0|   N|                Y|       Fast Shipping|Part exactly what...| 2015-08-31|\n","|         US|   32004835|R2KZBMOFRMYOPO|B00MVVIF2G|     563052763|Midea WHS-160RB1 ...|Major Appliances|          5|            1|          1|   N|                Y|          Five Stars|Love my refrigera...| 2015-08-31|\n","|         US|   25414497| R6BIZOZY6UD01|B00IY7BNUW|     874236579|Avalon Bay Portab...|Major Appliances|          5|            0|          0|   N|                Y|          Five Stars|No more running t...| 2015-08-31|\n","|         US|   36311751|R1MCXZFNF8E7Y0|B0033X29CI|     294467812|Danby  Freestandi...|Major Appliances|          1|            0|          0|   N|                Y|       Piece of Junk|It would not cool...| 2015-08-31|\n","|         US|   30920961|R3EMB3E3ODR6BW|B005R597HA|     183784715|Avanti 110-Volt A...|Major Appliances|          5|            2|          2|   N|                Y|Works awesome for...|Works awesome for...| 2015-08-31|\n","|         US|   52491265| RJTONVTTOPJ5S|B00MO6V8Y0|     960251524|      Danby products|Major Appliances|          5|            0|          0|   N|                Y|          Five Stars|exactly what I wa...| 2015-08-31|\n","|         US|   48166169|R21U5QZ2CQECUM|B00HT39QDI|     992475314|3 Pack Tier1 MSWF...|Major Appliances|          4|            0|          0|   N|                Y|          Four Stars|       AS advertised| 2015-08-31|\n","|         US|   50394924| RL2BBC51H89DH|B00LESFZ52|       1641606|True TSSU-60-16 6...|Major Appliances|          4|            0|          0|   N|                Y|but has poor insu...|It works as adver...| 2015-08-31|\n","+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n","only showing top 10 rows\n","\n"]}],"source":["from pyspark import SparkFiles\n","# Load in user_data.csv from S3 into a DataFrame\n","url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Major_Appliances_v1_00.tsv.gz\"\n","spark.sparkContext.addFile(url)\n","\n","df = spark.read.option('header', 'true').csv(SparkFiles.get(\"amazon_reviews_us_Major_Appliances_v1_00.tsv.gz\"), inferSchema=True, sep='\\t', timestampFormat=\"mm/dd/yy\")\n","df.show(10)"]},{"cell_type":"code","source":["# Drop duplicates and incomplete rows \n","print(df.count())\n","df = df.dropna()\n","print(df.count())\n","df = df.dropDuplicates()\n","print(df.count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjbtka-rkAcb","executionInfo":{"status":"ok","timestamp":1652481568977,"user_tz":420,"elapsed":4346,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"598e0188-dfa4-4818-88ba-86cb214734b4"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["96901\n","96888\n","96888\n"]}]},{"cell_type":"code","execution_count":29,"metadata":{"id":"E72hLCa7kT_D","executionInfo":{"status":"ok","timestamp":1652481573615,"user_tz":420,"elapsed":2487,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"47324d5a-f12d-4822-c512-bb15f1e514cc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["96888"]},"metadata":{},"execution_count":29}],"source":["rows=df.count()\n","rows"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"kRvAVaeWkXXn","executionInfo":{"status":"ok","timestamp":1652481576357,"user_tz":420,"elapsed":128,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c79b6ae-bc17-46fc-bb86-1014ccdb4fe1"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- marketplace: string (nullable = true)\n"," |-- customer_id: integer (nullable = true)\n"," |-- review_id: string (nullable = true)\n"," |-- product_id: string (nullable = true)\n"," |-- product_parent: integer (nullable = true)\n"," |-- product_title: string (nullable = true)\n"," |-- product_category: string (nullable = true)\n"," |-- star_rating: integer (nullable = true)\n"," |-- helpful_votes: integer (nullable = true)\n"," |-- total_votes: integer (nullable = true)\n"," |-- vine: string (nullable = true)\n"," |-- verified_purchase: string (nullable = true)\n"," |-- review_headline: string (nullable = true)\n"," |-- review_body: string (nullable = true)\n"," |-- review_date: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"qnF65nZ1kZAi"},"source":["## Transform DataFrame to fit date_table table"]},{"cell_type":"code","source":["customers_df = df.groupby(\"customer_id\").agg({\"customer_id\":\"count\"})\\\n","\n","customers_df.orderBy(\"count(customer_id)\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDoOkxY1Md0z","executionInfo":{"status":"ok","timestamp":1652481581599,"user_tz":420,"elapsed":2924,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"69f8d832-9469-4708-b674-e368134e3d61"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+------------------+\n","|customer_id|count(customer_id)|\n","+-----------+------------------+\n","|14664611   |1                 |\n","|21458971   |1                 |\n","|49857015   |1                 |\n","|31196655   |1                 |\n","|47319296   |1                 |\n","|13246430   |1                 |\n","|33085426   |1                 |\n","|52231496   |1                 |\n","|36173316   |1                 |\n","|37856551   |1                 |\n","|44309150   |1                 |\n","|1263434    |1                 |\n","|49355710   |1                 |\n","|23952664   |1                 |\n","|21737233   |1                 |\n","|45155320   |1                 |\n","|52628848   |1                 |\n","|19909263   |1                 |\n","|21782950   |1                 |\n","|27917440   |1                 |\n","+-----------+------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n","customers2_df = customers_df.withColumnRenamed(\"customer_id\", \"customer_id\")\\\n","                                     .withColumnRenamed(\"count(customer_id)\", \"customer_count\")\n","customers2_df.orderBy(desc(\"customer_count\")).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaeeqHUzNYnD","executionInfo":{"status":"ok","timestamp":1652481586681,"user_tz":420,"elapsed":2820,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"f7bb09f7-d2d7-422e-d554-19cc26f05700"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+--------------+\n","|customer_id|customer_count|\n","+-----------+--------------+\n","|   32962808|            16|\n","|   15489471|            15|\n","|    5695524|            15|\n","|   30396577|            14|\n","|   34553562|            13|\n","|   51042275|            12|\n","|   32230187|            12|\n","|   29748841|            12|\n","|   51139148|            12|\n","|   12914327|            11|\n","|   40382895|            11|\n","|   13649055|            11|\n","|   30544694|            11|\n","|   48159861|            11|\n","|   20862753|            11|\n","|   49444310|             9|\n","|   45671123|             9|\n","|   14384334|             9|\n","|   24715941|             9|\n","|   53074513|             9|\n","+-----------+--------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["customers2_df.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-MJHwOwOL-Y","executionInfo":{"status":"ok","timestamp":1652481591648,"user_tz":420,"elapsed":1600,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"8c9611f0-9b10-4630-eb67-ab5f255f75d8"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["91299"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["customers2_df.dtypes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-MyrK-Yl-AD","executionInfo":{"status":"ok","timestamp":1652481858591,"user_tz":420,"elapsed":118,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"84427d94-267b-490c-8f54-ba53a2670fcc"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('customer_id', 'int'), ('customer_count', 'bigint')]"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# Configuration for RDS instance\n","mode=\"append\"\n","jdbc_url = \"jdbc:postgresql://bigdatachallenge.cttxocti0nox.us-west-1.rds.amazonaws.com:5432/bd_db\"\n","config = {\"user\":\"root\",\n","          \"password\": \"postgres\",\n","          \"driver\":\"org.postgresql.Driver\"}"],"metadata":{"id":"P_7W2_BzOdWC","executionInfo":{"status":"ok","timestamp":1652481914762,"user_tz":420,"elapsed":122,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["customers2_df.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nQEM19fWPF-H","executionInfo":{"status":"error","timestamp":1652481916757,"user_tz":420,"elapsed":144,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"09ae4489-da25-42ee-be12-5f0bd423707f"},"execution_count":41,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-dca32394d2d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcustomers2_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'customers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o151.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:218)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:222)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:46)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"]}]},{"cell_type":"code","source":["product_df = df.select([\"product_id\",\"product_title\"])\n","product_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6v1sBgpMLe75","executionInfo":{"status":"ok","timestamp":1652476624524,"user_tz":420,"elapsed":323,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"0521a331-9cf1-4158-9e52-7deeb21e432e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+--------------------+\n","|product_id|       product_title|\n","+----------+--------------------+\n","|B0067WNSZY|FGGF3032MW Galler...|\n","|B002QSXK60|Best Hand Clothes...|\n","|B00EC452R6|Supco SET184 Ther...|\n","|B00MVVIF2G|Midea WHS-160RB1 ...|\n","|B00IY7BNUW|Avalon Bay Portab...|\n","|B0033X29CI|Danby  Freestandi...|\n","|B005R597HA|Avanti 110-Volt A...|\n","|B00MO6V8Y0|      Danby products|\n","|B00HT39QDI|3 Pack Tier1 MSWF...|\n","|B00LESFZ52|True TSSU-60-16 6...|\n","|B0149IJVPI|Magic: the Gather...|\n","|B002HT0958|Mini Portable Cou...|\n","|B006WOBNX6|1 X Dishwasher Ra...|\n","|B00NLPMOU0|Watson 1.5 ft AC ...|\n","|B00KJ07SEM|GE MWF SmartWater...|\n","|B001AT2ALM|Koldfront Ultra C...|\n","|B00JG8B42K|OnePurify RFC0800...|\n","|B00FRJ5EWS|2016 Life Water I...|\n","|B0052G14E8|Danby 0.7 cu.ft. ...|\n","|B00LOVVE9A|SPT UF-304SS Ener...|\n","+----------+--------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# Drop duplicates and incomplete rows \n","print(df.count())\n","df = df.dropna()\n","print(df.count())\n","df = df.dropDuplicates()\n","print(df.count())"],"metadata":{"id":"Eu9LG8yJU1wd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"W8gR8zpukhqW","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1652477237398,"user_tz":420,"elapsed":170,"user":{"displayName":"Chris Conner","userId":"04384723108664374069"}},"outputId":"1201248f-0ba7-44be-cd08-8dc5631baba5"},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-f9c2a4d91fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproduct_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'products'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o74.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:218)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:222)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:46)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"]}],"source":["product_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"iuV6hvV6kkcU","outputId":"646523af-9216-4048-d382-9656ad796579"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+--------------------+\n","|      date|         review_text|\n","+----------+--------------------+\n","|11/25/2016|1 check-in Love l...|\n","| 12/2/2016|Listed in Date Ni...|\n","|11/30/2016|1 check-in Listed...|\n","|11/25/2016|Very cool vibe! G...|\n","| 12/3/2016|1 check-in They a...|\n","|11/20/2016|1 check-in Very c...|\n","|10/27/2016|2 check-ins Liste...|\n","| 11/2/2016|2 check-ins Love ...|\n","|10/25/2016|1 check-in Ok let...|\n","|11/10/2016|3 check-ins This ...|\n","|10/22/2016|1 check-in Listed...|\n","|11/20/2016|The store has A+ ...|\n","|11/17/2016|1 check-in Listed...|\n","| 12/5/2016|This is such a cu...|\n","|11/13/2016|Beautiful eccentr...|\n","| 11/9/2016|1 check-in Listed...|\n","| 11/6/2016|Really love the v...|\n","|10/25/2016|1 check-in Check ...|\n","|10/15/2016|1 check-in Note: ...|\n","| 12/1/2016|So much aesthetic...|\n","+----------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import regexp_extract\n","review_df = review_df.withColumn(\"date\", regexp_extract(\"review_text\", \"\\d+/\\d+/\\d+\", 0))\\\n","      .withColumn(\"review_text\", regexp_extract(\"review_text\", \"\\d+/\\d+/\\d+(?:\\s)(.*)\", 1))\\\n","      .select([\"date\", \"review_text\"])\\\n","      .dropna()\n","review_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"nVPFb0PzkkTH","outputId":"a5134e88-4f1c-4f67-827d-5590007af543"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------------+\n","|      date|review_count|\n","+----------+------------+\n","| 8/21/2016|          16|\n","| 6/29/2016|          10|\n","| 8/19/2013|           2|\n","| 2/27/2015|           5|\n","| 7/31/2016|          13|\n","| 3/17/2014|           7|\n","|11/14/2015|          11|\n","| 6/10/2011|           1|\n","|10/10/2009|           1|\n","| 4/27/2014|           1|\n","| 3/27/2009|           1|\n","| 12/8/2011|           1|\n","| 2/21/2014|           2|\n","| 8/31/2015|          10|\n","| 1/15/2015|           3|\n","| 3/16/2012|           1|\n","|  8/9/2016|           4|\n","|11/24/2016|           1|\n","|  8/2/2014|           5|\n","| 3/23/2011|           1|\n","+----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["date_df = review_df.groupBy('date').agg({\"date\": \"count\"})\n","date_df = date_df.withColumnRenamed(\"count(date)\", \"review_count\")\n","date_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"E6P2i7rCkjte","outputId":"e0235c48-5dd4-4771-e204-5ed56bbd807a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------------+\n","|      date|review_count|\n","+----------+------------+\n","| 10/9/2016|          31|\n","| 9/18/2016|          30|\n","|11/20/2016|          27|\n","| 11/2/2016|          27|\n","| 12/2/2016|          26|\n","| 12/4/2016|          26|\n","| 9/15/2016|          25|\n","| 10/7/2016|          24|\n","| 11/6/2016|          24|\n","| 7/24/2016|          24|\n","| 4/17/2016|          23|\n","|10/25/2016|          23|\n","| 12/3/2016|          23|\n","| 12/1/2016|          23|\n","|  8/7/2016|          22|\n","| 6/27/2016|          22|\n","|  1/4/2016|          21|\n","| 1/17/2016|          21|\n","|11/21/2016|          21|\n","| 8/13/2016|          20|\n","+----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["date_df.orderBy(desc(\"review_count\")).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXQe3s_ElN2U"},"outputs":[],"source":[""]}],"metadata":{"colab":{"name":"big_data_appliances.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
=======
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrissconner84/big-data-challenge/blob/main/big_data_appliances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BtCOyQ3kChJ",
        "outputId": "6669473d-7833-4cf6-e80d-e02de521b874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Connecting to security.ub\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.2.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3FP8HaLokHui"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BigDataAppliances\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LSw6QzzkMkY",
        "outputId": "074dae3c-556c-404a-8d18-5bcac8aa8aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|         US|   16199106|R203HPW78Z7N4K|B0067WNSZY|     633038551|FGGF3032MW Galler...|Major Appliances|          5|            0|          0|   N|                Y|If you need a new...|What a great stov...| 2015-08-31|\n",
            "|         US|   16374060|R2EAIGVLEALSP3|B002QSXK60|     811766671|Best Hand Clothes...|Major Appliances|          5|            1|          1|   N|                Y|          Five Stars|        worked great| 2015-08-31|\n",
            "|         US|   15322085|R1K1CD73HHLILA|B00EC452R6|     345562728|Supco SET184 Ther...|Major Appliances|          5|            0|          0|   N|                Y|       Fast Shipping|Part exactly what...| 2015-08-31|\n",
            "|         US|   32004835|R2KZBMOFRMYOPO|B00MVVIF2G|     563052763|Midea WHS-160RB1 ...|Major Appliances|          5|            1|          1|   N|                Y|          Five Stars|Love my refrigera...| 2015-08-31|\n",
            "|         US|   25414497| R6BIZOZY6UD01|B00IY7BNUW|     874236579|Avalon Bay Portab...|Major Appliances|          5|            0|          0|   N|                Y|          Five Stars|No more running t...| 2015-08-31|\n",
            "|         US|   36311751|R1MCXZFNF8E7Y0|B0033X29CI|     294467812|Danby  Freestandi...|Major Appliances|          1|            0|          0|   N|                Y|       Piece of Junk|It would not cool...| 2015-08-31|\n",
            "|         US|   30920961|R3EMB3E3ODR6BW|B005R597HA|     183784715|Avanti 110-Volt A...|Major Appliances|          5|            2|          2|   N|                Y|Works awesome for...|Works awesome for...| 2015-08-31|\n",
            "|         US|   52491265| RJTONVTTOPJ5S|B00MO6V8Y0|     960251524|      Danby products|Major Appliances|          5|            0|          0|   N|                Y|          Five Stars|exactly what I wa...| 2015-08-31|\n",
            "|         US|   48166169|R21U5QZ2CQECUM|B00HT39QDI|     992475314|3 Pack Tier1 MSWF...|Major Appliances|          4|            0|          0|   N|                Y|          Four Stars|       AS advertised| 2015-08-31|\n",
            "|         US|   50394924| RL2BBC51H89DH|B00LESFZ52|       1641606|True TSSU-60-16 6...|Major Appliances|          4|            0|          0|   N|                Y|but has poor insu...|It works as adver...| 2015-08-31|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkFiles\n",
        "# Load in user_data.csv from S3 into a DataFrame\n",
        "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Major_Appliances_v1_00.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"amazon_reviews_us_Major_Appliances_v1_00.tsv.gz\"), inferSchema=True, sep='\\t', timestampFormat=\"mm/dd/yy\")\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IifpzN6zkN5Q"
      },
      "source": [
        "## Transform DataFrame to fit coffe_rating table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E72hLCa7kT_D"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kRvAVaeWkXXn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "OJgQqozHkYhz",
        "outputId": "8c0d6b3d-4118-49ac-c199-5cb61cee86f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+-----------------+\n",
            "|    coffee_shop_name|total_ratings|       avg_rating|\n",
            "+--------------------+-------------+-----------------+\n",
            "|Lola Savannah Cof...|            4|              5.0|\n",
            "|The Marvelous Vin...|           10|              5.0|\n",
            "|Mañana Coffee & J...|           33|4.848484848484849|\n",
            "|       Brian's Brew |           45|4.844444444444444|\n",
            "|Third Coast Coffe...|           56|4.821428571428571|\n",
            "|      Flitch Coffee |           28|4.821428571428571|\n",
            "|   Kowabunga Coffee |           16|           4.8125|\n",
            "|Venezia Italian G...|          200|             4.81|\n",
            "|      Legend Coffee |           28|4.714285714285714|\n",
            "|       Fleet Coffee |           57|4.701754385964913|\n",
            "|    My Sweet Austin |           31| 4.67741935483871|\n",
            "|         Dolce Neve |          100|             4.64|\n",
            "|       Holy Grounds |           30|4.633333333333334|\n",
            "|Anderson's Coffee...|          100|             4.62|\n",
            "|Apanas Coffee & B...|          136|4.580882352941177|\n",
            "|  Flat Track Coffee |           63|4.571428571428571|\n",
            "|Friends & Neighbors |           29|4.551724137931035|\n",
            "|Summermoon Coffee...|          100|             4.53|\n",
            "|      Corona Coffee |          100|             4.53|\n",
            "|    Live Oak Market |          100|             4.51|\n",
            "+--------------------+-------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import desc\n",
        "coffee_ratings_df = coffee_ratings_df.withColumnRenamed(\"count(coffee_shop_name)\", \"total_ratings\")\\\n",
        "                                     .withColumnRenamed(\"avg(num_rating)\", \"avg_rating\")\n",
        "coffee_ratings_df.orderBy(desc(\"avg_rating\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnF65nZ1kZAi"
      },
      "source": [
        "## Transform DataFrame to fit date_table table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8gR8zpukhqW"
      },
      "outputs": [],
      "source": [
        "review_df = df.select([\"review_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "iuV6hvV6kkcU",
        "outputId": "646523af-9216-4048-d382-9656ad796579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------+\n",
            "|      date|         review_text|\n",
            "+----------+--------------------+\n",
            "|11/25/2016|1 check-in Love l...|\n",
            "| 12/2/2016|Listed in Date Ni...|\n",
            "|11/30/2016|1 check-in Listed...|\n",
            "|11/25/2016|Very cool vibe! G...|\n",
            "| 12/3/2016|1 check-in They a...|\n",
            "|11/20/2016|1 check-in Very c...|\n",
            "|10/27/2016|2 check-ins Liste...|\n",
            "| 11/2/2016|2 check-ins Love ...|\n",
            "|10/25/2016|1 check-in Ok let...|\n",
            "|11/10/2016|3 check-ins This ...|\n",
            "|10/22/2016|1 check-in Listed...|\n",
            "|11/20/2016|The store has A+ ...|\n",
            "|11/17/2016|1 check-in Listed...|\n",
            "| 12/5/2016|This is such a cu...|\n",
            "|11/13/2016|Beautiful eccentr...|\n",
            "| 11/9/2016|1 check-in Listed...|\n",
            "| 11/6/2016|Really love the v...|\n",
            "|10/25/2016|1 check-in Check ...|\n",
            "|10/15/2016|1 check-in Note: ...|\n",
            "| 12/1/2016|So much aesthetic...|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "review_df = review_df.withColumn(\"date\", regexp_extract(\"review_text\", \"\\d+/\\d+/\\d+\", 0))\\\n",
        "      .withColumn(\"review_text\", regexp_extract(\"review_text\", \"\\d+/\\d+/\\d+(?:\\s)(.*)\", 1))\\\n",
        "      .select([\"date\", \"review_text\"])\\\n",
        "      .dropna()\n",
        "review_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "nVPFb0PzkkTH",
        "outputId": "a5134e88-4f1c-4f67-827d-5590007af543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------+\n",
            "|      date|review_count|\n",
            "+----------+------------+\n",
            "| 8/21/2016|          16|\n",
            "| 6/29/2016|          10|\n",
            "| 8/19/2013|           2|\n",
            "| 2/27/2015|           5|\n",
            "| 7/31/2016|          13|\n",
            "| 3/17/2014|           7|\n",
            "|11/14/2015|          11|\n",
            "| 6/10/2011|           1|\n",
            "|10/10/2009|           1|\n",
            "| 4/27/2014|           1|\n",
            "| 3/27/2009|           1|\n",
            "| 12/8/2011|           1|\n",
            "| 2/21/2014|           2|\n",
            "| 8/31/2015|          10|\n",
            "| 1/15/2015|           3|\n",
            "| 3/16/2012|           1|\n",
            "|  8/9/2016|           4|\n",
            "|11/24/2016|           1|\n",
            "|  8/2/2014|           5|\n",
            "| 3/23/2011|           1|\n",
            "+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "date_df = review_df.groupBy('date').agg({\"date\": \"count\"})\n",
        "date_df = date_df.withColumnRenamed(\"count(date)\", \"review_count\")\n",
        "date_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "E6P2i7rCkjte",
        "outputId": "e0235c48-5dd4-4771-e204-5ed56bbd807a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------+\n",
            "|      date|review_count|\n",
            "+----------+------------+\n",
            "| 10/9/2016|          31|\n",
            "| 9/18/2016|          30|\n",
            "|11/20/2016|          27|\n",
            "| 11/2/2016|          27|\n",
            "| 12/2/2016|          26|\n",
            "| 12/4/2016|          26|\n",
            "| 9/15/2016|          25|\n",
            "| 10/7/2016|          24|\n",
            "| 11/6/2016|          24|\n",
            "| 7/24/2016|          24|\n",
            "| 4/17/2016|          23|\n",
            "|10/25/2016|          23|\n",
            "| 12/3/2016|          23|\n",
            "| 12/1/2016|          23|\n",
            "|  8/7/2016|          22|\n",
            "| 6/27/2016|          22|\n",
            "|  1/4/2016|          21|\n",
            "| 1/17/2016|          21|\n",
            "|11/21/2016|          21|\n",
            "| 8/13/2016|          20|\n",
            "+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "date_df.orderBy(desc(\"review_count\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXQe3s_ElN2U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "big_data_appliances.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> 90879359662eccbbb85fbd82a83d418dffdf760c
